/*--------------------------------*- C++ -*----------------------------------*\
| =========                 |                                                 |
| \\      /  F ield         | OpenFOAM: The Open Source CFD Toolbox           |
|  \\    /   O peration     | Version:  1.6                                   |
|   \\  /    A nd           | Web:      www.OpenFOAM.org                      |
|    \\/     M anipulation  |                                                 |
\*---------------------------------------------------------------------------*/
FoamFile
{
    version     2.0;
    format      ascii;
    class       dictionary;
    location    "system";
    object      controlDict;
}
// * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * //

application       fastfoam;

libs              ("libuserfiniteVolume.so");

// startFrom         startTime;
startFrom         latestTime;

startTime         0.0;

startTimeMean     0.0;

startTimeCorr     0.0;

stopAt            endTime;

// endTime           2.0;
endTime           5.0;
// endTime           10.0;

// deltaT            0.0015; // mesh=medium1
deltaT            0.0012; // mesh=medium2

writeControl      adjustableRunTime;

writeInterval     0.02; 

// purgeWrite        0;
purgeWrite        2;

writeFormat       binary;
//writeFormat       ascii;

writePrecision    12;

//writeCompression  compressed;
writeCompression  uncompressed;

timeFormat        general;

timePrecision     12;

runTimeModifiable no;
// runTimeModifiable yes;

// adjustTimeStep    no;
adjustTimeStep    yes;

maxCo             0.2;
// maxCo             0.75;

maxDeltaT         25.0;

functions
{

	// I think we only need to send the processor directories back to my 
	// local workstation to do further post-processing.
	// Selectively get the files back into the host/master machine 
	// through the bash command: (I think this runs on each slave node)
	// > scp -r danny@master-node-on-Hyak:/scr/stf/dsale/fasFlume processor*/

	// render a visualization using VisIt or ParaView:
	// > "command to run VisIt on most current OpenFOAM time step"

	// now send your rendered images, and other important directories back
	// to your local workstation for further analysis
	// >  scp -r  processor*/ danny@local-workstation:/mnt/data-RAID-1/danny/Hyak-runs/fastFlume



  //   sysCall
  //   {
  //       type systemCall;
  //       functionObjectLibs ( "libsystemCall.so" );
        
  //       // executeCalls 0();
  //       // endCalls 0();
  //       // writeCalls 0();
        
  //       executeCalls 1("echo Execute system call before time iteration is done");
  //       endCalls 1("echo Finishing up with a system call, which is seems to be before the write call...");
  //       writeCalls 4(
  //       			"echo Writing to file call" 
        			 
  //       			"ls -l"
        			 
  //       			"if [ $MPI_PROCESS_CORE = 0 ]; then
  // 						echo "Do what the master needs to do."
		// 	      	fi"

		// 	      	"cp * $PBS_O_WORKDIR"
		// 			);

  //       // some calls we only want to execute on the MPI master node,
  //       // how to tell when on the master node?  ... need to know the 
  //       // environmental variables of the specific MPI library (OpenMPI)
  //       // The two $ will expand in the shell to the PID number of the running shell
  //       // 
  //       // you should be able to see the variable that has stored the number 
  //       // of the parallel process assigned by the MPI. Based on that number, 
  //       // you will know which one is the master process, which is either 0 or 1, 
  //       // depending on the number assignment scheme used by the MPI toolbox being used
  //       if [ $MPI_PROCESS_CORE = 0 ]; then
		// 	echo "Do what the master needs to do."

		// 	// I think commands to visualize (VisIt, ParaView), and
		// 	// transter over network to local workstation
		// 	// should execute here:

		// 	reconstructPar


		// fi

  //       outputControl outputTime;
  //       outputInterval 1;
  //   }

}
